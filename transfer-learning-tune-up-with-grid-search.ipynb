{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n### 🔍 Efficient Exploration: Fine-Tuning and Grid Search with EfficientNetV2B0\n\nWelcome to a pragmatic journey where we demystify the synergy between EfficientNetV2B0, fine-tuning, and grid search for image recognition tasks. No theatrics, just a logical exploration of methods to yield optimal results.\n\n### 🌐 EfficientNetV2B0: A Solid Starting Point\n\nWe kick things off with EfficientNetV2B0, a reliable architecture known for its efficiency in image classification. We'll explore its features and understand how it forms the backbone of our approach to achieving accurate predictions.\n\n### 🎯 Fine-Tuning Strategies: Focusing on the Last Ten Layers\n\nFine-tuning doesn't have to be a mystery. Discover a straightforward approach as we target the last ten layers of our base model. This precise adjustment allows us to adapt our model to the specific characteristics of our dataset, enhancing its ability to make accurate classifications.\n\n### 🔍 Grid Search: Systematic Parameter Exploration\n\nEnter grid search, a methodical way to navigate the hyperparameter landscape. We'll walk through how this systematic exploration helps us find the sweet spot for hyperparameter configuration, ensuring our model operates at its best without unnecessary complexity.\n\n### 🌐 Practical Logic: Simplifying Complexity\n\nThis isn't just theoretical; it's a practical guide for tackling image recognition challenges. We break down complex concepts into practical insights, making it accessible for anyone looking to optimize their models.\n\nElevate your image recognition game with a logical and systematic approach. Let's dive in! 🚀\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_file = (f\"/kaggle/input/digit-recognizer/test.csv\")\ntest_csv = pd.read_csv(test_file)\ntrain_file = (f\"/kaggle/input/digit-recognizer/train.csv\")\ntrain_csv = pd.read_csv(train_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualiztion","metadata":{}},{"cell_type":"code","source":"row_index = 78  \nlabel = train_csv.loc[row_index, 'label']\npixels = train_csv.loc[row_index, 'pixel0':'pixel783'].values\nimage = pixels.reshape(28, 28)  \n\n# Display the image\nplt.figure(figsize=(3,3))\nplt.imshow(image, cmap='gray')\nplt.title(f\"Label: {label}\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing for train_csv","metadata":{}},{"cell_type":"code","source":"X = train_csv.loc[:, 'pixel0':'pixel783'].values / 255.0\ny = train_csv['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_reshaped = X.reshape(-1, 28, 28, 1)\nX_train_rgb = np.repeat(X_train_reshaped, 3, axis=-1)\nX_train_rgb.shape,y.unique().shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train_rgb, y, test_size=0.2, random_state=42)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing for the test_csv","metadata":{}},{"cell_type":"code","source":"X_test_csv = test_csv.loc[:, 'pixel0':'pixel783'].values / 255.0\nX_test_csv_reshaped = X_test_csv.reshape(-1, 28, 28, 1)\nX_test_csv_rgb = np.repeat(X_test_csv_reshaped, 3, axis=-1)\nX_test_csv_rgb.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 10\ninput_shape = (28, 28, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We have created a create_model function that has EfficientNetV2B0 as it's base layer followed by additional convolutional neural network (CNN) layers. We unfreezed the last 10 layers of EfficientNetV2B0 for our notebook. However, you can unfreeze more or less number of layers for fine-tuning with respect to your problem statement.\nAlso, we have taken input_shape, learning_rate, num_hidden_units, conv_units, dense_units, num_classes as arguments which will perform grid search on our of which input_shape and num_classes are fixed arguments(as we don't need to perform grid search on those).","metadata":{}},{"cell_type":"code","source":"def create_model(input_shape, learning_rate, num_hidden_units, conv_units, dense_units, num_classes):\n    # Create the base EfficientNetV2B0 model\n    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n    base_model.trainable = True  # Set the entire base model to be trainable\n\n    # Freeze all layers except the last ten\n    for layer in base_model.layers[:-10]:\n        layer.trainable = False\n\n    # Define the input layer\n    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n\n    # Pass the input through the base model\n    x1 = base_model(inputs, training=False)\n    x1 = GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x1)\n\n    # Create the CNN-based model\n    model_cnn = tf.keras.models.Sequential()\n    model_cnn.add(Conv2D(conv_units, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n    model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n    model_cnn.add(Flatten())\n    model_cnn.add(Dense(dense_units, activation='relu'))\n    model_cnn.add(Dense(num_classes, activation='softmax'))\n\n    # Pass the input through the CNN-based model\n    x2 = model_cnn(inputs)\n\n    # Concatenate the outputs of the two models\n    x = tf.keras.layers.concatenate([x1, x2])\n\n    # Add additional layers as needed\n    x = Dense(num_hidden_units, activation='relu')(x)\n\n    # Output layer\n    outputs = Dense(num_classes, activation='softmax', name=\"output_layer\")(x)\n\n    # Create the final model\n    final_model = tf.keras.Model(inputs, outputs)\n\n    # Compile the model\n    final_model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        metrics=[\"accuracy\"]\n    )\n\n    return final_model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Here we have used EfficientNetV2B0 as our base model for transfer learning though you can increase the complexity of the base model by using other models labeled  from EfficientNet B0 to B7. You can use this link for the reference https://keras.io/api/applications/efficientnet/.\n> \n> EfficientNet models are known for their ability to achieve state-of-the-art performance on various computer vision tasks while being computationally efficient. The scaling approach used in their design allows for effective model generalization across different tasks and datasets. Researchers often choose a specific EfficientNet variant based on the requirements of their task, balancing the need for accuracy with computational efficiency.","metadata":{}},{"cell_type":"markdown","source":"### 🚀 Fine-Tuning Brilliance: Exploring Model Excellence with Grid Search in Keras 🤖","metadata":{}},{"cell_type":"markdown","source":"> We have commented out this code snippet though you can uncomment it and run the grid search to get the best hyperparameters with reference to your model's architecture.\n>\n> We made things simpler by creating a new function, create_model_fixed, that includes fixed details like input_shape and num_classes. This helps keep the code neat and organized. The function is then used in our model setup for grid search, where we explore different hyperparameters. After the search, the best parameters and their corresponding accuracy are printed. This approach makes it easier to manage both fixed and adjustable parts of the model.\n>\n> While running the code you might come across a DeprecationWarning that indicates that the KerasClassifier class you are using is deprecated.The recommendation is to use keras-tuner.\n\n> You can use the link for reference :\n> https://www.tensorflow.org/tutorials/keras/keras_tuner","metadata":{}},{"cell_type":"code","source":"# fixed_params = {\n#     'input_shape': input_shape,\n#     'num_classes': num_classes, \n# }\n\n# # Create a new function with fixed parameters\n# def create_model_fixed(learning_rate, num_hidden_units, conv_units, dense_units):\n#     return create_model(**fixed_params, learning_rate=learning_rate, num_hidden_units=num_hidden_units,\n#                         conv_units=conv_units, dense_units=dense_units)\n\n# model = KerasClassifier(build_fn=create_model_fixed, epochs=10, batch_size=32, verbose=0)\n\n# # Define the hyperparameters for grid search\n# param_grid = {\n#     'learning_rate': [0.001, 0.01],\n#     'num_hidden_units': [64, 128],\n#     'conv_units': [32, 64],\n#     'dense_units': [64, 128]\n# }\n\n# # Perform grid search without k-fold cross-validation\n# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy',cv = 2 ,verbose=1)\n# grid_result = grid_search.fit(X_train, y_train)\n\n# # Print the best parameters and corresponding accuracy\n# print(\"Best parameters found: \", grid_result.best_params_)\n# print(\"Best accuracy found: \", grid_result.best_score_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The provided code utilizes k-fold cross-validation to improve the model's performance evaluation. By splitting the data into different folds and training the model on one subset while validating on another, we obtain more reliable accuracy scores. This approach helps ensure the model's effectiveness across diverse portions of the dataset, leading to a more trustworthy assessment of its overall performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnum_splits = 5\nkf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n\n# Lists to store the cross-validation results\nacc_scores = []\nfold_number = 1\ntrained_models = {}\ny_train_array = np.array(y_train)\n\n# Perform k-fold cross-validation\nfor train_index, val_index in kf.split(X_train):\n    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n    y_train_fold, y_val_fold = y_train_array[train_index], y_train_array[val_index]\n\n    # Assuming you have optimal hyperparameters determined through grid search\n    best_learning_rate = 0.001\n    best_num_hidden_units = 128\n    conv_units = 64\n    dense_units = 128\n\n    # Create and compile the model with the best hyperparameters\n    model = create_model(input_shape=input_shape,\n                         learning_rate=best_learning_rate,\n                         num_hidden_units=best_num_hidden_units,\n                         conv_units=conv_units,\n                         dense_units=dense_units,\n                         num_classes=num_classes)\n\n    # Train the model on the current fold\n    history = model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, verbose=0, validation_data=(X_val_fold, y_val_fold))\n\n    # Evaluate the model on the validation fold\n    _, accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n    print(f\"Fold {fold_number}: Validation Accuracy = {accuracy}\")\n\n    # Plot loss and accuracy curves\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='Train')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # Confusion Matrix\n    y_val_pred = model.predict(X_val_fold)\n    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n    y_val_true_classes = y_val_fold\n\n    cm = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix - Fold {}'.format(fold_number))\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n\n    trained_models[fold_number] = model\n    fold_number += 1\n    acc_scores.append(accuracy)\n    print(\"--------------------------------------------\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation check","metadata":{}},{"cell_type":"code","source":"y_val_array = np.array(y_val)\nval_loss, val_accuracy = model.evaluate(X_val, y_val_array)\nprint(f'Validation Loss: {val_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The loop iterates through each trained model stored in the trained_models dictionary. For each model, predictions are made on the X_test_csv_rgb dataset, and these individual predictions are appended to the individual_predictions list. Finally, the mean prediction is computed by taking the average of the individual predictions along the specified axis (axis=0).\n> \n> This ensemble strategy can be particularly effective when the individual models capture different aspects of the data or have complementary strengths, contributing to a more reliable and stable prediction.","metadata":{}},{"cell_type":"code","source":"individual_predictions = []\n\n# Iterate through the trained models and make predictions\nfor fold_number, model in trained_models.items():\n    predictions = model.predict(X_test_csv_rgb)\n    individual_predictions.append(predictions)\n\n# Calculate the mean of all predictions\nmean_prediction = np.mean(individual_predictions, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions_file = (f\"/kaggle/input/digit-recognizer/sample_submission.csv\")\nsample_submission = pd.read_csv(submissions_file)\nsample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids = range(1, len(mean_prediction) + 1)\n# Create a DataFrame with 'ImageId' and 'Label' columns\nsubmission_df = pd.DataFrame({'ImageId': image_ids, 'Label': mean_prediction.argmax(axis=1)})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submitting to kaggle","metadata":{}},{"cell_type":"code","source":"sample_submission['Label'] = submission_df['Label']\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)\nsample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scope for Improvement\n1. We can do Rotations and other Image Pre-processing steps on input images and see if it helps on the accuracy\n2. Increment EfficientNetB0 to higher versions and check the accuract\n3. Check the mis-classified numbers and add similar looking sets to data or duplicate them to see if that helps on accuracy.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}